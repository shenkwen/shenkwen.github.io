{"pages":[],"posts":[{"title":"ggplot与数据可视化图形语法","text":"ggplot2是一个功能强大的可视化工具，更加值得学习的是其设计思想，一套清晰的可视化设计逻辑。 ggplot的设计理念，以统计学家Leland Wilkinson的可视化著作‘The Grammar of Graphics’中的“图形语法”做为基础。这本书对数据图表的设计思路进行了相对来说具有普适性的抽象和总结，试图形成一套“图形语法”。如同人类语言是由名词、动词、形容词等元素按照一定的规则组合而成，图表(graphics)同样可以抽象为由不同语法元素按某些规则逐“层”叠加而成，数据可视化正是将数据映射为这些语法元素，从而完成数据特征到视觉元素特征的转换，制作出有意义的图表。 因此学习ggplot最关键的内容便是：作为图层(layer)的各种语法元素(Grammatical Elements) ，以及将数据信息映射为各种视觉元素(Aesthetic Mappings)，由这一思路开始逐步展开的学习将更加系统。 语法元素图表的语法元素主要包括： Data：也就是我们将要绘制的数据 Aesthetic Mappings：图表中各种视觉/美学元素，我们要将数据的各类信息映射到这些元素上，比如x/y轴、大小、颜色、透明度、线条类型等等； Geometries： 几何层，展现数据的几何图形类型，比如点、线、柱。如果用过excel作图，对这一层也许最熟悉——excel的作图首先便是选择作散点图、线图还是直方图等； Facets：分面，用多个子图表分组显示数据； Statistics： 统计变换层，用于辅助理解数据的统计量； Coordinates： 坐标系 其中前三是必不可少的核心元素，其他为可选元素用以丰富图表信息或调整图表细节。ggplot的作图过程，就是将上述的元素逐“层”叠加的过程。下面用简单的例子显示图层逐步叠加的过程，从而对上述的各类元素以及叠加过程有一个感性的认识。（这里只会大致介绍一些作图的基本思想、技巧，具体到ggplot中一些函数的细节不作过多展开，可自行查询相关文档。） 语法元素的叠加过程以iris数据为例，我们从最简单的绘图工作开始，逐层丰富图表元素。 基础元素 + 几何层用上述Data + Aesthetic Mappings + Geometries三个核心元素，绘制一副最基础的散点图：12ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() 首先，使用ggplot函数指定data和mapping两个参数，分别对应于数据和视觉映射。 使用以gemo_为前缀的一系列函数指定图表的几何类型，比如本例中 gemo_point将图表指定为散点图。图层的叠加使用“+”连接。 需要重点说明的是视觉映射，也就是参数mapping的设置，其值用aes函数设定各种图形属性，通过它将我们想要表现的数据信息映射为恰当的视觉元素。在本例中由于绘制散点图，必须指定x坐标和y坐标 —— 也就是各个散点的位置。 图1的视觉映射指定了x和y坐标两个元素，体现在散点图中就是数据集中每个case对应点的位置。对于某些种类的图只需要指定一个轴，比如直方图： 123# geom_histogram将图表指定为直方图ggplot(data=iris, mapping=aes(x=Sepal.Length)) + geom_histogram() 当然我们可以将更多数据信息映射到视觉元素上，比如用颜色的深浅表现出Petal.Length这一变量的大小。表现在ggplot代码中，就是将aes函数中color参数的值，设为iris数据中的Petal.Length列： 12ggplot(data=iris, mapping=aes(x=Sepal.Length, y=Sepal.Width, color=Petal.Length)) + geom_point() 加入分面(facets)分面绘图可方便不同子集的数据在视觉上的比较。如图2，将一副图表拆分为并排的三列子图表，分别绘制数据的不同子集，也就是三种鸢尾花（对应与iris数据中的Species列）。1234# facet_grid指定为网格分面，参数格式为“行～列”，若只有指定行或列，则相应的列或行使用“.”ggplot(data=iris, mapping=aes(x=Sepal.Length, y=Sepal.Width, color=Petal.Length)) + geom_point() + facet_grid(. ~ Species) 加入统计(statistics)层有时候我需要在图表中加一些图形以增加相关统计量的展现，丰富图表信息，这一功能由以“stat_”为前缀的一系列函数实现。比如最常见的散点图中的回归拟合曲线，图3中我们使用stat_smooth来实现这一功能。其实很多情况下，统计量和几何形状是对应的，许多统计量也都有对应的几何形状(如stat_boxplot对应于geom_boxplot, stat_density对应geom_density)。 12345# 使用stat_smooth，在三幅子图表中分别加入回归拟合曲线，用method参数指定回归方法为线性回归。ggplot(data=iris, mapping=aes(x=Sepal.Length, y=Sepal.Width, color=Petal.Length)) + geom_point() + facet_grid(. ~ Species) + stat_smooth(method='lm') 调整坐标通过以“coord_”为前缀的一系列函数对图表的坐标进行调整。比如图4中，使用coord_fixed将图表坐标系的x轴和y轴调整为相同缩放级别： 123456# 使用coord_fixed调整坐标。ggplot(data=iris, mapping=aes(x=Sepal.Length, y=Sepal.Width, color=Petal.Length)) + geom_point() + facet_grid(. ~ Species) + stat_smooth(method='lm') + coord_fixed()","link":"/2016/01/11/%E5%8F%AF%E8%A7%86%E5%8C%96/ggplot%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%9B%BE%E5%BD%A2%E8%AF%AD%E6%B3%95/"},{"title":"ggplot语法元素之几何层(geom)之一：数据分布问题，从点到线","text":"大多数人绘制数据图表的最初尝试通常是在excel中，其基础知识是理解各类图表的功能，比如柱图、饼图、条形图、散点图，这些不同的形状就是ggplot中所谓的几何层、一系列以“geom_”开头的函数，比如geom_point(点图)、geom_bar(柱图”)、geom_histogram(直方图)等。 分别介绍各类图表用法的材料很多，最初在阅读这些材料时多少会感觉混乱，各个图表都有不同的功能、参数、用法，不容易记忆。这里尝试换一种线索，对于不同的作图任务，都从最基本的点图开始，通过不断改进展现效果，串联起各类图表及其最优方法。之所以从点图开始，因为其体现了可视化一种很直觉的做法 —— 将数据中每个case映射为图像中的一个点。 为了梳理各种庞杂的可视化图形，这里将可视化任务粗疏地总结三种类型： 首先一大类是对数据分布情况的可视化，包含离散/连续数据，以及一维/多维数据。 其次是序列性数据的可视化，典型的是时间序列。 最后整理一下对已经统计数据进行可视化的各种做法。如果说以上两类皆带有探索性分析的性质，旨在通过图形深化对数据的观察和理解，这一种是对现成的统计数据做图形化展现，目的更多是对信息的包装和传递。 第一篇先从一维连续数据分布情况的可视化开始，通过对点图的不断优化，串联起一系列图表类型。 一维连续变量： 从“点”到“柱”到“密度曲线”通常来说一维连续变量的可视化目标是描述其分布状态，如何通过视觉效果反映这一信息？如同开头所介绍，可以先尝试将每个case绘制为一个点，对于一维连续变量来说，所有case会落在一条连续的数轴上。ggplot提供了geom_rug这一图形（图1-1），将每个case按照其取值映射为坐标轴上的一条短线，其疏密程度就描述了数据的大致分布。显然，这一图形过于简陋，通常它也不会被单独使用，而是作为辅助图形丰富图表的表现力。 123# 本节使用了R内置的quakes数据集ggplot(quakes, aes(depth)) + geom_rug(color=\"red\", alpha=0.2) 连续变量的一种常见的可视化方法是将其离散化。如下图所示，首先划分出若干个宽度为10的区间（binwidth=10），然后将每个case绘制为相应区间上的点并堆叠起来，每个区间里点的数量反映其分布情况，视觉上则体现为点堆叠的高度。 1234# dotplot绘制，其中binwidth参数用来指定区间的宽度ggplot(quakes, aes(depth)) + geom_dotplot(dotsize=0.7, binwidth=10, fill=\"#619CFF\", color=NA) + geom_rug(color=\"red\", alpha=0.2) 当然这种做法并不常用，毕竟不可能去数每个区间上有多少点。既然这里的有效视觉信息是高度，那么用柱形表现更合适，即直方图（histogram）（图1-3）。可以注意到无论是dotplot或直方图，都需要指定binwidth参数，因为离散化需要先划分区间，即分桶（bin)，因此需要指定区间宽度（binwidth），或指定区间数量（bins)。 1234# 绘制直方图，其中binwidth参数用来指定区间宽度ggplot(quakes, aes(depth)) + geom_histogram(binwidth=10, fill=\"#619CFF\", color=\"white\") + geom_rug(color=\"red\", alpha=0.2) 此外也可以通过geom_freqpoly，用线条勾勒出数据的分布形状： 12345# freqpolyggplot(quakes, aes(depth)) + geom_histogram(binwidth=10, fill=\"#619CFF\", color=\"white\", alpha=0.2) + geom_rug(color=\"red\", alpha=0.2) + geom_freqpoly(binwidth=10, color=\"#F87667\") freqpoly绘制的曲线和直方图表达的信息并无本质不同。讲到直方图和曲线，统计课中经常由此引入概率密度曲线的内容。基于有限观测案例来拟合密度曲线，通常使用的是非参方法核密度估计(KDE)*。图1-4在直方图的基础上叠加了kde density plot。注意这幅图由于要配合密度曲线的展现，映射在直方图y轴上的统计量要调整为密度..density..。在ggplot中一些常见的统计量使用..xxx..这样的格式表示，比如..count..（计数）、 *..density..（密度）等。默认情况下直方图y轴的统计量为计数。 12345# 图1-5，密度曲线ggplot(quakes, aes(depth)) + geom_histogram(aes(y=..density..), binwidth=10, fill=\"#619CFF\", color=\"white\") + geom_rug(color=\"red\", alpha=0.2) + geom_density(color=\"#F87667\") 一维连续变量分布的分组比较很多时候需要通过可视化对不同组case的分布进行对比，以R内置的diamonds数据为例，如何用可视化来表示不同“cut”分类的case，其价格的分布？同样延续本文的思路，首先从点图开始，将每个case绘制为一个点，其x坐标为cut，y坐标为price。注意这里使用的是散点图geom_point，留意一下dotplot和point的区别：dotplot会对先对连续变量进行离散化即分桶，而point会精确的按照其x/y轴的值绘制为一个点。 12345# 对数据做一些预处理，为了展示效果更好，砍掉一部分长尾数据。diamonds2 = diamonds[diamonds$price &lt; 10000, ]# 使用geom_point绘制ggplot(diamonds2, aes(x = cut, y = price)) + geom_point() 如同前文介绍geom_rug时类似，点密密麻麻的分布在若干个一维数轴上，无法观察具体分布。针对这一情况，ggplot提供了geom_jitter这一图形，会在映射到x/y轴位置的变量上加入小幅随机偏移，使散点随机错开，形成一个带状区域，造成视觉上的疏密效果，由此可以观察case的分布情况。注意下图只在宽度即x轴上加入随机偏移，因为x轴为分类变量并不是具体数值，其偏移并不造成实质影响，只是为了形成一个带状区域。 123# 由于数据量太大，这里缩小了每个点的大小（size)ggplot(diamonds2, aes(x = cut, y = price)) + geom_jitter(size=0.1, width=0.3, height=0) # 注意这里在高度上不加入偏移(height=0) 但是无论如何，通过散点的疏密只能获得直觉上的感受，连续变量还是需要通过密度曲线展现详细的分布情况： 123# 将分组信息映射为颜色（color=cut）ggplot(diamonds2, aes(x=price, color=cut)) + geom_density() 12ggplot(diamonds2, aes(x=price, fill=cut)) + geom_density(color=NA, position=\"stack\") 上面两幅图显示，当分组数比较多时无可避免会造成密度曲线重叠，视觉效果依然不够理想，这种情况可以考虑以下解决方案。首先，利用上文用到的dotplot，即对连续变量进行分桶后将case绘制为相应分桶上的点，并堆叠起来，整体上形成数据的分布形状。（图2-5） 1234# 使用dotplot绘制多组case的分布情况# 注意参数中使用了binaxis=y，表示这里对连续变量的分桶发生于映射在y轴上的变量（即price）。ggplot(diamonds2, aes(x = cut, y = price)) + geom_dotplot(aes(color=cut), binaxis='y', stackdir = 'center', binwidth=12) 如同上文通过dotplot和直方图引入密度曲线，这里也可以通过dotplot引入另一种密度曲线的呈现方式，即小提琴图geom_violin(图2-6)，每个“小提琴”的形状展现了每组数据的分布情况，越宽的地方代表数据分布越多，类似于将密度图垂直并对称化展现，其实质上就是对拟合的密度曲线进行相应变形。 12345# 为了对比dotplot和小提琴图的效果，这里将两种图形叠加在一起# 小提琴图中设置了adjust参数，该参数用来控制绘制密度图kde方法的brandwithggplot(diamonds2, aes(x = cut, y = price)) + geom_dotplot(aes(color=cut), binaxis='y', stackdir = 'center', binwidth=12, alpha=0.05) + geom_violin(aes(color=cut), fill=NA, adjust=0.5) 上图显示，小提琴图拟合出的平滑密度曲线的形状，和dotplot绘制出的点的分布形状一致，不过在面积上有所区别。这是由于小提琴图的主要目标是反应数据的分布密度，默认情况下各分类的小提琴图面积会归一化为1，因此其面积不体现case规模。这个可以通过scale参数修改。 1234# 这里的小提琴图中调整了scale参数，从而使小提琴图的面积与数据规模匹配ggplot(diamonds2, aes(x = cut, y = price)) + geom_dotplot(aes(color=cut), binaxis='y', stackdir = 'center', binwidth=12, alpha=0.05) + geom_violin(aes(color=cut), fill=NA, adjust=0.5, scale='count') 在一些专业语境下是需要增加更为专业的图形辅助提供统计信息，比如箱线图（geom_boxplot），以观察中位数、四分位距、异常值等。 123ggplot(diamonds2, aes(x = cut, y = price)) + geom_dotplot(binaxis='y', stackdir = 'center', binwidth=12, alpha=0.03) + geom_boxplot(aes(color=cut), fill=NA, width=0.5) 总结从绘制点图这一最直观的可视化图形开始，我们不断调整优化、寻找最优的图形，串联起histogram、dotplot、rug、freqpoly、density、violin、boxplot等一系列几何图形。希望能够通过这途径对各种可视化图形有更清晰的理解。接下来我们还会使用相同的思路，进行二维数据、序列数据等相关作图的分析。","link":"/2016/02/11/%E5%8F%AF%E8%A7%86%E5%8C%96/ggplot%E8%AF%AD%E6%B3%95%E5%85%83%E7%B4%A0%E4%B9%8B%E5%87%A0%E4%BD%95%E5%B1%821/"},{"title":"ggplot语法元素之几何层(geom) - 之三：数据分布问题，分类数据","text":"分类数据的分布就是各个分类的频数统计，继续延续之前的思路，从尝试绘制点图开始，然而对于分类数据来说大部分情况下点图不适用，这里的尝试只是用来引入更合适的图表以加深理解。 一维分类数据与对连续数据离散化之后的做法相同，先将每个case绘制为相应分类上的一个点： 123# 使用了geom_dotplot这一几何图形。注意这里仅做为一个引子，实际情况下不会这样使用ggplot(msleep, aes(vore)) + geom_dotplot(dotsize=0.8) 上图中，每个分类的频数大小，反映为点的数量，及其堆叠所呈现出的“高度”。当然这种做法既不美观也不实用，尤其当case量很大时。与连续数据使用的直方图类似，用“高度”展现频数会使用柱图（geom_bar）： 123# 绘制柱状图。ggplot(msleep, aes(x = vore)) + geom_bar(fill=\"#619CFF\", width = 0.7) # fill和width分别调整柱子的颜色和宽度 注意上面柱状图的视觉映射中，只指定了x=vore，并未指定y，因为柱状图默认映射为y的统计量为计数（count）。在ggplot中一些常见的统计量使用..xxx..*这样的格式表示，比如..count..（计数）、 *..density..（密度）等。如果我们希望柱的高度（即映射为y的变量）用来表示占比，可以使用以下形式： 1234# 将柱图的y轴数值调整为占比ggplot(msleep, aes(x = vore, y = ..count.. / sum(..count..))) + geom_bar(fill=\"#619CFF\", width = 0.7) + ylab(\"ratio\") # ylab调整y轴的标签 二维分类数据分类变量同样也涉及到分组比较的问题，即二维分类数据的可视化。同样从尝试绘制点图出发，其实这里很容猜到如果使用geom_point，每个点会在相应的位置上重叠起来： 12ggplot(diamonds, aes(x=clarity, y=cut)) + geom_point() 针对这种情况，需要从视觉上区分每个分类上重叠了多少个点。ggplot提供了另外一种geom_count图形，可以用点的面积大小，来反映重叠量的多少： 1234# 这里geom_count的视觉映射中，size映射为..prop..，也就是占比# 既然是占比就需要指定分母，group=1意味着占比的分母是整体caseggplot(diamonds, aes(x=clarity, y=cut, color=cut)) + geom_count(aes(size=..prop.., group=1)) 在介绍二维连续数据分布的可视化时，提到使用geom_bin2d通过热力图的方式绘制离散化后的二维数据。这种方法自然适用本身就是离散的分类数据： 1234# geom_bin2d的group含义和geom_count一致ggplot(diamonds, aes(x=clarity, y=cut)) + geom_bin2d(aes(fill=..density.., group=1)) + scale_fill_gradient(low = 'lightyellow', high = 'red') 这种展现至二维平面上的方法，其目的之一是观察映射至x/y轴上的两个变量的关系，这里作为案例数据的diamonds，碰巧cut和clarity两列都是定序变量，取值有顺序关系，映射至二维平面上或许能传递某些的信息（比如案例中的两个变量其实是存在正相关关系的）。然而对于单纯的分组变量来说这种比较通常没有意义。若想较精确地反映数值上的比较，常见的还是分组后的柱形图，一般会将另一个分组变量映射为填充颜色： 123# 默认情况下，position=\"stack\"ggplot(diamonds, aes(x=clarity, fill=cut)) + geom_bar() 当涉及到对柱图的分组时，就存在着如何安排位置的问题，默认情况下position=”stack”，也就是堆叠起来（如上图）。另外常见的还有dodge, fill等，其效果通过绘制出来的图表可以了解。 123# position=\"dodge\"ggplot(diamonds, aes(x=clarity, fill=cut)) + geom_bar(position=\"dodge\") 123# position=\"fill\"ggplot(diamonds, aes(x=clarity, fill=cut)) + geom_bar(position=\"fill\") geom_bar与geom_col如果有在Excel中绘图的经验，换到使用ggplot柱图进行可视化时可能会不习惯：Excel中绘制柱图需要事先算好频数，而geom_bar是默认计算出count。如果希望使用excel的方式，直接用计算好的频数绘制，需要使用geom_col这一图形。下面代码绘制的图形与上文一致。 12345# 事先计算好countvore.count = data.frame(table(msleep$vore))# 使用geom_colggplot(vore.count, aes(x = Var1, y = Freq)) + geom_col(fill=\"#619CFF\", width = 0.7) 在前面的介绍中，无论一维二维连续数据还是分类数据，核心都在解决描绘case“分布”的问题，但是我们也会碰到像geom_col这样需要将已经算好的统计数据（比如上述的频数/占比，或者其他统计数据）直接展示的问题，不同于分布问题的探索性分析，这一类问题的可视化目标除了给予一种视觉上的直观感受，很多时候是信息展示和视觉包装，在易于理解的基础上，做到清晰、美观，方便观看者理解数据所传递的结论。","link":"/2016/03/26/%E5%8F%AF%E8%A7%86%E5%8C%96/ggplot%E8%AF%AD%E6%B3%95%E5%85%83%E7%B4%A0%E4%B9%8B%E5%87%A0%E4%BD%95%E5%B1%823/"},{"title":"ggplot语法元素之几何层(geom) - 之二：数据分布问题，从点到面","text":"前一篇针对一维连续数据分布情况的可视化，通过从点图出发、不断引入新图形优化展现效果的思路，介绍了一系列图表类型。本文继续延续这一思路，对二维连续数据分布状况的可视化进行梳理。 使用点图对一维连续数据进行可视化时，曾提到过通过疏密观察数据分布，这一点在二维数据中更加明显：一维数据的疏密体现在一条线性区域上，而二维数据的疏密体现在一个平面上。 通过散点疏密表现分布12345# 以ggplot2内置的diamonds数据框为例，该数据一共有53940个casedata(diamonds)# 绘制散点图ggplot(diamonds, aes(x = carat, y = price)) + geom_point() 当数据case量较大时，散点会大量重叠造成视觉混乱。遇到该问题可使用一些小技巧解决，比如将散点透明度降低，或者缩小散点半径，形成区分度更高的疏密效果。 123# 技巧1：大量降低散点透明度并扩大散点半径，形成疏密的视觉效果(图2)ggplot(diamonds, aes(x = carat, y = price)) + geom_point(alpha=0.01, size=5) 123# 技巧2：使用半径较小的点，同样可形成疏密效果ggplot(diamonds, aes(x = carat, y = price)) + geom_point(shape='.') 然而如同前一篇所述，散点的疏密在视觉上的区分度依然局限，需要更加专业的图形辅助展现数据分布情况。 通过离散后展现分布如同使用直方图通过离散化展现一维分布，同样可以将二维分布的每个维度都进行离散化后，将平面划分为网格来展现二维数据分布，每个网格的case数量由连续色谱标示，形成热力图，ggplot中提供了geom_bin2d和geom_hex两种图形，分别实现方格和六边形蜂箱图： 12345678# 首先准备后续需要使用的数据：对R内置的txhousing数据进行清洗，为获得更好的展示效果txhousing2 = txhousing[(txhousing$inventory &lt; 20) &amp; !is.na(txhousing$inventory) &amp; !is.na(txhousing$median), ]# 使用geom_bin2d来绘制二维分布密度的热力图# 这里使用scale_fill_gradient目的是调整色谱，相关用法可见其他介绍ggplot(txhousing2, aes(median, inventory)) + geom_bin2d(aes(fill=..density..), bins=30) + scale_fill_gradient(low = 'lightyellow', high = 'red') 1234# 使用geom_hex绘制六边形蜂巢图ggplot(txhousing2, aes(median, inventory)) + geom_hex(aes(fill=..density..)) + scale_fill_gradient(low = 'lightyellow', high = 'red') 二维密度图前一篇中介绍了一维数据的密度曲线，用一条曲线的高低起伏描述数据分布，与之类似也有二维密度图(geom_density_2d)，通过等高线的疏密、颜色的深浅等视觉元素展现数据分布的密度情况。以下分别实现“等高线”及其填充颜色后的效果: 1234# 绘制二维密度图ggplot(txhousing2, aes(median, inventory)) + geom_point(alpha=0.2, size=2) + geom_density_2d(aes(color=..level..)) 12345# 二维热力密度图 # 使用了统计层的相关函数stat_，其用法后续介绍ggplot(txhousing2, aes(median, inventory)) + stat_density_2d(aes(fill=..level..), geom=\"polygon\") + scale_fill_gradient(low = 'lightyellow', high = 'red')","link":"/2016/03/08/%E5%8F%AF%E8%A7%86%E5%8C%96/ggplot%E8%AF%AD%E6%B3%95%E5%85%83%E7%B4%A0%E4%B9%8B%E5%87%A0%E4%BD%95%E5%B1%822/"},{"title":"ggplot语法元素之几何层(geom) 之四：序列数据","text":"序列数据的可视化目标通常为展现一种变化趋势，一般是一组数据或指标Y随另一组数据X的变化而产生的变化趋势，最常见的如时间序列是某个指标随着时间这个自变量而变化的趋势，基于此可以将序列的概念扩大，只要那个X是有序的，都不妨将这类问题看作序列数据的可视化。 序列数据当然也可以用前面介绍的点图、柱图表现，但是既然注重于展现变化趋势，最常见的还是线图。 线图及其变化形式以下为线图及其几种变化形式： 12345# 筛选部分数据作为绘图案例数据economics2 &lt;- economics[economics$date &gt;= as.Date(&quot;2012-01-01&quot;) &amp; economics$date &lt;= as.Date(&quot;2014-12-01&quot;), ]ggplot(economics2, aes(date, psavert)) + geom_line() 12ggplot(economics2, aes(date, psavert)) + geom_step() 12ggplot(economics2, aes(date, psavert)) + geom_area() 分组比较线图的分组比较，需要特别指定一个参数group，指定哪个变量中取值相同的case作为一组用线连接。比如以下案例中分年度比较指标每个月的变化，其中group指定为year，即同一年份的数据用线连接。 123456# 数据预处理，加入year和month两列economics2$year = as.character(format(as.Date(economics2$date), &quot;%Y&quot;))economics2$month = format(as.Date(economics2$date), &quot;%m&quot;)# 分年比较每月的指标变化，除了指定color=year，还要指定group=yearggplot(economics2, aes(month, uempmed, group=year, color=year)) + geom_line() 12ggplot(economics2, aes(month, uempmed, group=year, linetype=year)) + geom_line() 123# geom_area的分组比较ggplot(economics2, aes(month, uempmed, fill = year, group=year)) + geom_area(position = 'dodge')","link":"/2016/04/05/%E5%8F%AF%E8%A7%86%E5%8C%96/ggplot%E8%AF%AD%E6%B3%95%E5%85%83%E7%B4%A0%E4%B9%8B%E5%87%A0%E4%BD%95%E5%B1%824/"},{"title":"ggplot语法元素之数据","text":"对ggplot各语法元素以及图层叠加过程有了框架性的认识之后，便可以按照此路径开始逐步熟悉各个语法元素。首先最基础的数据层。 数据就是我们要可视化的对象，与其他分析任务相似，对于可视化工作来说数据预处理的过程同样必要, 其主要目标是对数据进行整理和变形，将需要映射为视觉元素的信息变为相应的feature，从而使后续的作图思路更加清晰。 数据表格结构对绘图的影响看下面两幅对iris数据的可视化图： 在图1中，我们试图在两个子图中，分别比较三种不同鸢尾花花萼和花瓣的长度和宽度，第一幅为长度、第二幅为宽度；每幅子图中散点分成了三列，对应三种花；同时每一列的散点分为两种颜色，对应花萼和花瓣。这幅图在视觉上表现了十分丰富的数据信息：每一列内部有花萼、花瓣长（宽）度的比较，横向有不同种类鸢尾花的对比，跨子图可发现数据在长度、宽度两个测量指标上的不同分布情况。 图2中，将三种鸢尾花分成三个子图，每个子图中散点分位两种颜色分别对应花萼和花瓣，而散点在x轴和y轴的位置分别对应花萼（瓣）的长度和宽度。 在思考如何画出以上两幅图之前，回忆一下之前对视觉元素映射的简单介绍，x轴、y轴和颜色都属于视觉元素(aesthetic)，分别由数据的一列指标映射而来，图表的分面（facet）也是基于相应的指标。在图1中，显然x轴是由鸢尾花的种类（Species）映射而来，而映射到y轴、颜色和分面的指标，在原iris数据表中并不存在。 也就是说为了画出这幅图，需要对原数据集进行预处理和变形，使数据格式与我们想要可视化的信息匹配，具体到这一幅图，原iris数据表需调整为如下形式： part measure value Species id Sepal Length 5.1 setosa 1 Sepal Width 3.5 setosa 1 Petal Length 1.4 setosa 1 Petal Width 0.2 setosa 1 …… …… …… …… Petal Width 1.8 virginica 150 原数据表中，每一行为一个鸢尾花样本，花萼、花瓣的长、宽分别为四个feature，在变形后的数据中，每个样本（新增id列来识别样本）的四列feature被转置为四行，同时四个变量名被分裂为part, measure两部分。具体实现代码如下： 123456iris['id'] = seq(nrow(iris)) # 新增一行id用来标识样本# 使用reshape2模块对数据进行变形(reshap2的使用方法不做展开，可自行查询相关资料)。library(reshape2)iris2 = melt(iris, id.vars=c('id', 'Species'))cols = colsplit(iris2[['variable']], '[.]', c('Part', 'Measure'))iris2 = cbind(iris2, cols) 在iris2数据表的基础上，图1的实现便十分直观。 12345# 图1的绘制# 注：这里使用了geom_jitter，为加入了噪音的随机抖动的散点图。由于横轴为离散变量，如果直接使用geom_point会造成散点大量重叠，具体可见散点图一节的说明。ggplot(iris2, aes(x = Species, y = value, color=Part)) + geom_jitter(width=0.3, height=0) + facet_grid(. ~ Measure) 同样的，图2的绘制需要对数据进行进一步变形，按照图1思路，我们需要将iris2 中长度和宽度恢复为两列feature，具体实现如下: 1234567# 在iris2基础上，进一步将数据集变形为iris3# 将measure的值还原为Length和Width两列属性iris3 = dcast(iris2, formula=id+Species+Part ~ Measure, value.var='value')ggplot(iris3, aes(x=Length, y=Width, color=Part)) + geom_point() + facet_grid(.~Species) 总结起来，进行数据可视化首先要明确我们需要可视化的核心信息，在ggplot作图中，这一过程表现为我们需要明确将哪些feature映射为恰当的视觉元素，此时原有数据格式可能无法直接使用这些feature，便需要对数据进行预处理和变形。这看似与作图关系不大，但若没处理好，后期的作图过程将非常不顺畅，甚至无法实现我们要的效果。","link":"/2016/01/16/%E5%8F%AF%E8%A7%86%E5%8C%96/ggplot%E8%AF%AD%E6%B3%95%E5%85%83%E7%B4%A0%E4%B9%8B%E6%95%B0%E6%8D%AE/"},{"title":"ggplot语法元素之视觉映射(aesthetic-mapping)","text":"可视化的本质就是将数据信息转化为视觉信息，比如: 一个柱子的高度，对应了某一统计值的大小，统计值越大，柱子高度就越高；一个点在x轴上的位置，对应了这个case某个特征的取值；图形的不同颜色，代表着数据的不同分类；这里的高度(其本质是y轴)/x轴/颜色等都是视觉元素，除此之外还有大小、形状等。在ggplot2中，这些视觉元素都在aes函数中指定为数据中的某一列变量，最终作为ggplot函数中mapping参数的值，代表着将数据mapping为aes(thetic)。 常用的视觉元素这里将一些常用的视觉映射元素整理如下： Aesthetic(即aes函数的参数) 含义 适用变量类型 x x轴位置 连续、离散 y y轴位置 连续、离散 size 点的直径、线条的宽度 连续 alpha 透明度 连续、离散 color 点的颜色、其他颜色的轮廓颜色 离散、连续（色谱） fill 填充颜色 离散、连续（色谱） label 点或坐标轴标签 离散 shape 点形状 离散 linetype 线条类型 离散 本文重点讨论如何合理实现数据变量向视觉元素的映射。需要考虑两个因素： 变量类型：主要关注离散型变量和连续性变量的视觉映射。 图表类型：也就是散点、折线、柱状等图表相对应的视觉元素映射。 对于第二点，在学习几何层、也就是各种类型图表的作图方法时自然会涉及到。因此这里主要就第一点做一些展开。 离散和连续变量的视觉映射先看下面一组图的比较，我们分别鸢尾花种类（离散型变量）映射为透明度、形状、形状+颜色，显然第三幅的视觉区分效果最好： 虽然在上面的表中列出了多种适用于离散型变量映射的视觉元素，但是效果有好有坏，一般来说，对于离散型变量来说视觉区分度以及信息表达效率从低到高排序： 形状, 线条粗细(折线图) &lt; 单色色谱/灰度/透明度、线条形状(折线图) &lt; 颜色 , 文字标签 同样，对于连续型变量，各类视觉元素的区分度和信息表达效率也各有不同，从低到高如下： 颜色 &lt; 单色色谱 &lt; 灰度 &lt; 大小/面积 &lt; 长度/高度 &lt; 坐标轴位置 同样通过以下两幅图的比较展现连续型变量的视觉映射效果。除了把变量Sepal.Length和Sepal.Width分别映射至x轴和y轴外，这里还想在图中展现Petal.Length这一变量，图2-1使用颜色（注意在缺省情况下连续型变量映射为颜色时会自动使用单色色谱）， 图2-2使用面积，显然通过面积更容易获得对变量大小的直观感觉（虽然在散点过多的情况下不够美观），色谱则需要通过图例的辅助转换为对数值大小的感知。然而，离散变量最有效率的映射对象仍然是x/y坐标轴的位置，所以在作图时我们通常把最关键的变量映射到x/y轴位置上，其他视觉元素作为辅助信息。","link":"/2016/01/23/%E5%8F%AF%E8%A7%86%E5%8C%96/ggplot%E8%AF%AD%E6%B3%95%E5%85%83%E7%B4%A0%E4%B9%8B%E8%A7%86%E8%A7%89%E6%98%A0%E5%B0%84/"},{"title":"ggplot语法元素之几何层(geom) - 之五：辅助图形","text":"除了散点图、柱图、线图等主要图形，有时需要使用一些辅助图形来增加信息量、补充统计量、优化视觉效果，以下简单梳理这些配角图形，在此之前需要首先说明ggplot中多图层叠加的问题。 多图层叠加在可视化的过程中有时需要将一些统计量体现在图上，这一功能的实现通过叠加多个几何图层来实现（即叠加多个geom_…函数）。 通常情况下，ggplot函数中指定的data层和mapping层确定了全局参数，即后续geom层会默认继承ggplot函数中指定的参数，除非进行了单独设定。 下面的案例在散点图中通过叠加一层新的geom_point，添加了平均值信息。新添加的图层重新指定了data， mapping则继承自ggplot函数，当然继承的前提是新数据在变量名上需保持一致。 123456789# 本节案例使用R内置的mpg数据，对数据进行一点预处理mpg['year'] = as.factor(mpg$year)# 计算displ, hwy的平均值，并修正变量名使其与原数据保持一致mpg.mean = aggregate(mpg[c(\"displ\", \"cty\")], list(mpg$year), mean)names(mpg.mean)[1] = 'year'# 将平均值叠加到散点图中ggplot(mpg, aes(displ, cty, color=year)) + geom_point() + geom_point(data=mpg.mean, shape=15, size=5, alpha=0.5) 辅助线在图形中增加辅助线，可以更加明确地进行观察和比较，不过需要注意最好将辅助线弱化，防止出现视觉上喧宾夺主的情况。 1234567# geom_hline: 水平辅助线，相应的可以mapping的参数为yintercept# geom_vline: 垂直辅助线，相应的可以mapping的参数为xintercept# 水平和垂直辅助线均设置为虚线（linetype=2)，以弱化展现ggplot(mpg, aes(displ, cty, color=year)) + geom_point() + geom_hline(data=mpg.mean, aes(yintercept=cty, color=year), linetype=2) + geom_vline(data=mpg.mean, aes(xintercept=displ, color=year), linetype=2) 辅助线还可以起到分割坐标、便于观察的效果，比如在时间序列数据中，可以用来分割时段： 12345# 设定需要分割的时间段# 增加垂直线以分割时间段ggplot(economics, aes(date, pce)) + geom_line(color='red') + geom_vline(data=presidential, aes(xintercept=start), linetype=2, alpha=0.5) 误差在发表专业论文时，经常要通过误差线error bar来表现测量的误差或者置信区间，叠加在主要图形比如柱图、线图、点图上。绘制误差线需要提供数据的上届、下届，在geom_errorbar这一图形上，分别映射为xmin/ymin, xmax/ymax。 12345678910# 数据准备： Mean均值，Upper上界，Lower下界iris.mean = data.frame( Species = c(\"setosa\", \"versicolor\", \"virginica\"), Mean = c(5.006, 5.936, 6.588), Upper = c(4.905824, 5.789306, 6.407285), Lower = c(5.106176, 6.082694, 6.768715))# 在柱图上绘制误差线ggplot(iris.mean, aes(x=Species, y=Mean, fill=Species)) + geom_col(width=0.5) + geom_errorbar(aes(ymin=Lower, ymax=Upper), width=0.1)","link":"/2016/04/14/%E5%8F%AF%E8%A7%86%E5%8C%96/ggplot%E8%AF%AD%E6%B3%95%E5%85%83%E7%B4%A0%E4%B9%8B%E5%87%A0%E4%BD%95%E5%B1%825/"},{"title":"余弦距离的LSH函数","text":"海量数据的相似项发现一文讨论了Jaccard距离及其对应的局部敏感哈希(LSH)函数族 —— 最小哈希(minhash)。Jaccard距离适用于集合类数据的距离度量，比如可以将一篇文章看作是由若干个k长度字符串构成的集合，因此在例如互联网上相似文章的发现这样的应用中，可对海量的文章事先进行最小哈希分桶以降低计算量。 不过如今各类机器学习算法大行其道，更多时候需要解决的是向量数据之间的距离，最常见的距离计算方法为余弦距离和欧式距离。本文介绍余弦距离对应的局部敏感哈希。 余弦距离给定一个n维空间上的两个向量A和B，需要找到一类LSH函数族，使得这两个向量哈希后分桶相同的概率，与这两个向量的夹角θ形成某种反向关系 —— θ越小（余弦距离越大），两者分桶相同的概率越大。 如下图，与A和B垂直的两个向量A’和B’，可以将由A和B张成的超平面，划分为四个区域： 落在红色区域内的向量，与A和B的余弦距离一定都为正 落在蓝色区域内的向量，与A和B的余弦距离一定都为负 落在绿色区域内的向量，与A的余弦距离为正，与B的余弦距离为负 落在黄色区域内的向量，与A的余弦距离为负，与B的余弦距离为正 假设在超平面上随机生成一个向量，并分别计算与A和B的余弦距离，由于随机向量落在红色和蓝色区域的概率为(360 -2θ)/360即(180-θ)/180，落在绿色和黄色区域的概率为θ/180，因此两个余弦距离符号相同的概率为180-θ/180， 符号相反的概率为θ/180 。 继续扩大到整个N维空间，根据基本的线性代数原理可知，对于任意一个N维向量，其与A或B的余弦距离的正负情况，与该向量在A和B张成的超平面上的投影向量相同，也就是说上面得出的结论可以扩大到整个N维空间。 因此可以构建如下的哈希函数族：hash(A) = sign(cos(A, X))，其中X为随机生成的N维向量，cos为余弦距离。根据上文推算，该函数是(d1, d2, 180-d1/180, 180-d2/180)敏感的。同样如上一篇文章所述，可以将该函数进行放大处理，获更优化的效果。 sketch在实际操作中，X并不需要真的从全部向量空间中随机选取，为了计算方便，可以将X设定为一个由+1和-1组成的随机向量，那么向量与X做内积运算的过程，就是该向量每个维度取值之间的加减运算。相关结论证明，这类向量已经足够随机，同时也降低了计算量。注意可能出现内积为0的情况，可随机地将其替换为+1或-1。 如果选取N个由+1和-1组成的随机向量，使用这些向量对A进行上述的N轮哈希，那么由这些哈希值构成的结果称为向量A的sketch(“草图”?“梗概”?)。 举例来说，假设要对四维空间上的两个向量A = [5, 5, 3, 4], B =[3, 2, -2, -2]进行上述哈希过程。在四维空间上由+1和-1组成的向量一共有2^4=16个： [-1, -1, -1, -1] [-1, -1, -1, 1] [-1, -1, 1, -1] …… [ 1, 1, 1, -1] [ 1, 1, 1, 1] 将16个向量与A和B分别计算内积并取sign，那么对应的A可B的sketch分别为： [-1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1][-1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1] 在这16次哈希中，分桶相等的次数为10，根据以上给出的该哈希函数性质，可估计A和B的夹角约为67.5度。实际上可计算两者的夹角为73.9度，估计值非常接近，证明上述哈希方法的有效性。","link":"/2016/05/27/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E4%BD%99%E5%BC%A6%E8%B7%9D%E7%A6%BB%E7%9A%84LSH%E5%87%BD%E6%95%B0/"},{"title":"海量数据的相似项发现","text":"（本文内容整理自斯坦福公开课cs226 Massive Data Mining - finding similar items） 对于一个规模为n的集合，去发现集合中相似度在某个范围内的相似项，处理这一问题最直觉的做法就是两两比较计算相似度，对于海量数据来说，其O(n^2)的复杂度显然无法接受。 两两比较最大的问题的在于有些项目根本无需去比较，比如两篇微博，如果字面上都没什么重合的片段，那就无需去比较相似度；又比如，相似度如果定义为欧氏距离，那就没必要拿空间一端的点和另一端的点去计算距离。 然而如果不去比较，怎么事先知道无需比较？这一问题的解决思路是事先划出若干个相似度可能比较接近的项目子集，只在子集内部寻找相似项，从而降低计算规模。 假设每个子集的平均规模为m，那么该问题的复杂度为(n/m)×m^2 = m×n, 当m足够小该问题复杂度为线性。 而划分子集的关键在于降低成本的同时尽量减少错误率：对于前者，可通过某种哈希算法对集合进行分桶，以在线性时间内完成划分工作；对于后者，则涉及到哈希函数的选择，其原则是两个项目哈希之后分到同一个桶的概率，应该正比于两者的相似度。 因此哈希函数的选择是解决该问题的核心所在。相似度算法有很多种，不同相似度也对应着不同的哈希函数。下面以jaccard相似度为例，介绍最小哈希是如何实现上述目标的。 Jaccard相似度和最小哈希Jaccard相似度可以度量两个有限集合的相似度，其计算十分简单，对于两个有限集合A和B，其相似度为A和B中共有元素的数量，除以两者中全部元素数量。例如，当A={a, b, c}, ， B={a, c, d}，两者共有元素数量为2(a和c），全部元素数量为4(a, b, c, d)，那么两者的Jaccard相似度为2/4=0.5。 对于这种相似度，可设计最小哈希（minhash）函数来拟合。其步骤简述为如下： 首先对构建一个由全部元素组成的向量，并随机乱序。比如对于A，B两个集合，其全部元素组成的向量为[a, b, c, d]，假设其随机乱序后的结果为[c, a, b, d]。 然后，对于每个集合，找到其所有元素在随机乱序后向量中的第一个元素的位置，比如对于A，其所有元素在[c, a, b, d]向量中第一个元素为c，因此其最小哈希值为1，同样B的最小哈希值也为1。同样的，如果随机乱序后的向量为[b, a, d, c]，那么A的最小哈希值为1，而B的最小哈希值为2（因为B集合中并不存在b元素）。可以很容易的证明，A和B最小哈希值相同的概率等于两者的Jaccard相似度。 以上简述了最小哈希的基本思想，其具体实现过程需要一些技巧。比如随机乱序的实现，对于海量数据来说，要枚举出所有元素构建向量，还要对其随机排序，通常也无法实现。在实际操作中可通过哈希函数实现，将每个元素映射为hash code，可以将该数字作为其在一个“虚拟向量”中的索引值，然后通过哈希函数将索引值映射为桶编号，该桶编号可认为是某种乱序后的新索引值，也就是最小哈希后的编号。 最小哈希的放大处理了解最小哈希后，实际上可以开始划分子集的工作了。然而对于两个Jaccard相似度为p的集合，其一次最小哈希划分至同一个子集的概率为p，还是有1-p的概率不会划分到一起。这时一种自然的想法是进行n轮哈希(每一轮都更换实现随机乱序的哈希函数)，那么n轮哈希后两者至少有一次被划分到一起的概率为1 -(1-p)^n， 比如对于5轮哈希来说，相似度为0.8的两个集合，至少有一轮被划分到一起的概率为99.968%。 多轮哈希解决了伪负例的问题，即应该划分到一起的项目没有划分到一起，然而却没有解决伪正例的问题，即不应该划分到一起的项目被划分到一起。比如对于5轮哈希，不同相似度的项目至少有一轮被划分到一起的概率可绘制如下曲线，可以发现相似度仅有0.25的两个项目，这一概率也高达75%。 上述方法还有一个问题，每次最小哈希最终都会以一个元素作为哈希值，因此高频元素很容易最终成为分桶依据，导致分桶不均匀，拖累整体计算速度。 这两个问题可通过以下方法解决。将n轮最小哈希后的值合并，形成签名，签名完全相同的两个项目分至同一个子类（桶）中，这样相似度为p的两个项目进入相同子类的概率为p^n。对上述过程再次进行m轮，那么这两个项目在m轮中有一轮分入同一子类中的概率为 1-(1 - p^n) ^m，假设n=5, m=20，同样绘制不同p的两个项目至少有一次划入同一桶的概率： 此时曲线变成了s形，可以发现相比前面的方法，这里在降低伪阴例的同时也降低了伪阳例的概率。 局部敏感哈希（LSH）最小哈希函数实际上是一大类函数中的一种，在前文中层提到，我们要找的这个哈希函数应该与项目之间的相似度存在相关关系。准确的说，我们要找的是这样一类哈希函数： 令d1 &lt; d2，对于距离小于d1的两个项目，其哈希值相等（即分入同一子集）的概率必定大于p1；对于距离大于d2的两个项目，其哈希值相等的概率必定小于为p2 —— 这一类函数称为(d1, d2 ,p1, p2)敏感函数族 。其中，前者保证相似项分入同一子集的概率较高（即避免伪负例），后者保证不相似项分入同一子集的概率较低（即避免伪正例）。而为了实现这两个目标，最好是d1, d2能够接近，而p1, p2尽量远离，假定d1=0.4, d2=0.6, p1=0.05, p2=0.99，这便意味着，距离只要小于0.4，那么他们分入同一子集的概率最多只有0.05，而距离只要大于0.6，这一概率一定大于0.99。 上文介绍的最小哈希就属于这一种函数族，因为最小哈希一定是(d1, d2, 1-d1, 1-d2)敏感的。假定d1=0.4, d2=0.8, 那么最小哈希函数就是（0.4， 0.8， 0.6， 0.2）敏感的，显然这里的p1, p2是不够理想的。 而经过放大处理后的哈希过程，依然符合上述定义。比如当n=5, m=20(每轮哈希值由5次最小哈希组成，一共进行20轮)时，假定d1=0.4, d2=0.8, 可以算出p1 = 0.003，p2 = 0.967，也就是这一哈希函数族是（0.4，0.8， 0.003， 0.967）敏感的，显然通过放大处理后，我们可以在缩小d1和d2之差的同时，显著地放大了p1和p2的距离，实现了极大的优化。这就是多轮哈希的目标所在，当然这种优化是以牺牲计算速度伪代价的。 本文通过最小哈希进入局部敏感哈希的概念。局部敏感哈希适用于集合数据的jaccard距离，而其他比如余弦距离、欧式距离也有相应的一些哈希函数族，这些留待后续介绍。","link":"/2016/05/13/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E7%9A%84%E7%9B%B8%E4%BC%BC%E9%A1%B9%E5%8F%91%E7%8E%B0/"},{"title":"cython实践记录,以KNN改写为例(1)： 矩阵计算","text":"近期开发工作中尝试使用cython对python代码进行了一些优化，对cython有了一些基础的认识。以下以KNN算法的改写为例，在jupyter notebook记录了逐步改写以提升代码性能的过程, 从而梳理了cython的一些常见功能，形成cython进阶的一些尝试。 $(\"#ipynb\").load( function() { console.log($(\"#ipynb\").contents().find(\"body\").find(\"#notebook\")); document.getElementById('ipynb').height=$(\"#ipynb\").contents().find(\"#notebook\").height()+100; })","link":"/2016/07/05/coding/knn%E7%9A%84cython%E5%87%BD%E6%95%B0%E6%B5%8B%E8%AF%95/"}],"tags":[{"name":"R","slug":"R","link":"/tags/R/"},{"name":"ggplot2","slug":"ggplot2","link":"/tags/ggplot2/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"cython","slug":"cython","link":"/tags/cython/"}],"categories":[{"name":"可视化","slug":"可视化","link":"/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"coding","slug":"coding","link":"/categories/coding/"}]}